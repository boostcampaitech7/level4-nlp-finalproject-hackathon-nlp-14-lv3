{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì €ì¥\n",
    "* ì´ˆê¸° RAW ë°ì´í„°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"âŒ LLAMA_CLOUD_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "parser = LlamaParse(result_type=\"text\")  # âœ… \"text\"ë¡œ ì„¤ì •í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜\n",
    "\n",
    "async def process_pdf(folder_path=\"./ë©í\"):\n",
    "    \"\"\"ë¹„ë™ê¸°ë¡œ í´ë” ë‚´ì˜ PDF íŒŒì¼ ì½ê¸° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    pdf_files = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "                \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"ğŸ“„ ì²˜ë¦¬í•  PDF: {pdf_path}\")\n",
    "        documents = await parser.aload_data(pdf_path)\n",
    "        extracted_text = \"\\n\\n\".join([f\"--- Page {i+1} ---\\n\\n{doc.text}\" for i, doc in enumerate(documents)])\n",
    "        relative_path = os.path.relpath(pdf_path, folder_path)  # ê¸°ì¤€ í´ë” ëŒ€ë¹„ ìƒëŒ€ ê²½ë¡œ\n",
    "        txt_relative_path = os.path.splitext(relative_path)[0] + \".txt\"  # í™•ì¥ìë§Œ ë³€ê²½\n",
    "        txt_filepath = os.path.join(folder_path, txt_relative_path)  # ì›ë³¸ í´ë” êµ¬ì¡° ìœ ì§€\n",
    "        with open(txt_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "            \n",
    "        print(f\"âœ… ì €ì¥ ì™„ë£Œ: {txt_filepath}\")\n",
    "        \n",
    "folder_path = \"./ë©í\"\n",
    "asyncio.run(process_pdf(folder_path=folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def save_pdf_images(pdf_path, base_folder):\n",
    "    \"\"\"PDFì˜ ê° í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        pdf_dir = os.path.splitext(os.path.relpath(pdf_path, base_folder))[0]  # ì›ë³¸ í´ë” êµ¬ì¡° ìœ ì§€\n",
    "        image_folder = os.path.join(base_folder, pdf_dir + \"_images\")  # \"_images\" í´ë”ì— ì €ì¥\n",
    "        os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
    "            im = page.to_image(resolution=300)  # 300 DPI í•´ìƒë„ ì„¤ì •\n",
    "            image_path = os.path.join(image_folder, f\"page_{i+1}.png\")\n",
    "            im.annotated.save(image_path, format=\"PNG\")\n",
    "\n",
    "        print(f\"ğŸ“· ì´ {len(pdf.pages)}ê°œì˜ í˜ì´ì§€ê°€ ì´ë¯¸ì§€ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {image_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬\n",
    "* RoboFlowë¥¼ í™œìš©í•´, image ë°ì´í„° graph crop ì§„í–‰ ë° í•™ìŠµ ì§„í–‰\n",
    "  * ì‚¬ìš©ëª¨ë¸ yolov11\n",
    "* í˜„ì¬ëŠ” Project ì œê±° ìƒíƒœ(ë°ì´í„° ë³´ì™„ì„ ìœ„í•´)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the inference-sdk\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "API_KEY = os.getenv(\"ROBOFLOW_API_KEY\", \"Your API Key\")\n",
    "CLIENT = InferenceHTTPClient(\n",
    "    api_url=\"https://detect.roboflow.com\",\n",
    "    api_key=API_KEY\n",
    ")\n",
    "folder_path = \"/content/drive/MyDrive/ë©í\"\n",
    "idx = 0\n",
    "\n",
    "for root, _, files in os.walk(folder_path):\n",
    "  folder_name = os.path.basename(root)\n",
    "  parent_folder = os.path.dirname(root)\n",
    "  output_folder = os.path.join(parent_folder, f\"{folder_name}\")\n",
    "  for file in files:\n",
    "    if file.endswith(\".png\"):\n",
    "      file_path = os.path.join(root, file)\n",
    "      print(f\"Processing: {file_path}\")\n",
    "      result = CLIENT.infer(file_path, model_id=\"graph_pr/2\")\n",
    "      if 'predictions' in result and result['predictions']:\n",
    "        img = Image.open(file_path)\n",
    "        img_width, img_height = img.size\n",
    "        for graph in result['predictions']:\n",
    "          # ì¢Œí‘œ ë³€í™˜\n",
    "          x, y, width, height = map(int, [graph['x'], graph['y'], graph['width'], graph['height']])\n",
    "          left = max(0, x - width // 2)\n",
    "          upper = max(0, y - height // 2)\n",
    "          right = min(img_width, x + width // 2)\n",
    "          lower = min(img_height, y + height // 2)\n",
    "          cropped_img = img.crop((left, upper, right, lower))\n",
    "          output_file = os.path.join(output_folder, f\"graph_{idx}.jpg\")\n",
    "          cropped_img.save(output_file)\n",
    "          print(f\"Saved: {output_file}\")\n",
    "          idx += 1\n",
    "      else:\n",
    "          print(f\"No graphs detected in {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬ 2\n",
    "* ìœ„ì—ì„œ Yoloë¥¼ í†µí•´ ì¶”ì¶œëœ ë°ì´í„°ë¥¼ GPT4o-minië¥¼ í™œìš©í•˜ì—¬ ê·¸ë˜í”„ ìš”ì•½ ë° ë¶„ì„ ì§„í–‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API ì„¤ì •\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ìµœìƒìœ„ í´ë” ê²½ë¡œ\n",
    "base_folder = \"./tasty-data/ë©í/\"\n",
    "\n",
    "# ì´ë¯¸ì§€ ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_image(image_path, company_name):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Base64 ì¸ì½”ë”©\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"ë„ˆëŠ” ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ê°€ì•¼. {company_name}ì˜ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ê·¸ë˜í”„ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ íŒë‹¨í•©ë‹ˆë‹¤. ë§Œì•½ ê·¸ë˜í”„ê°€ í¬í•¨ë˜ì—ˆë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°í™”ëœ ê·¸ë˜í”„ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤:\n",
    "1.ì¶• ì •ë³´: Xì¶•ê³¼ Yì¶• ë¼ë²¨ ë° ë²”ìœ„\n",
    "2.ì¶”ì„¸ ë° íŒ¨í„´: ì„ í˜•, ì§€ìˆ˜, ì‚¬ì¸íŒŒ ë“±ì˜ ìœ í˜• ì‹ë³„\n",
    "3.í•µì‹¬ í¬ì¸íŠ¸: ê·¹ì (ìµœê³ ì , ìµœì €ì ), êµì°¨ì , ë§ˆì»¤ ë“±\n",
    "4.í†µì°°: ê·¸ë˜í”„ê°€ ë‚˜íƒ€ë‚´ëŠ” ì˜ë¯¸\n",
    "Chain-of-Thought ë¶„ì„:\n",
    "ê·¸ë˜í”„ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¦…ë‹ˆë‹¤:\n",
    "1.ì¶• ì •ë³´: Xì¶•ê³¼ Yì¶•ì˜ ë¼ë²¨ ë° ë²”ìœ„ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.\n",
    "2.ì¶”ì„¸ ë° í˜•íƒœ: ê·¸ë˜í”„ê°€ ì„ í˜•ì¸ì§€, ì§„ë™í•˜ëŠ”ì§€, ì§€ìˆ˜ì  ì¦ê°€ë¥¼ ë³´ì´ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "3.í•µì‹¬ í¬ì¸íŠ¸: ìµœê³ ì , ìµœì €ì , êµì°¨ì , ë§ˆì»¤ ë“±ì„ ì°¾ì•„ ê°•ì¡°í•©ë‹ˆë‹¤.\n",
    "4.í†µì°°: ê·¸ë˜í”„ê°€ ë‚˜íƒ€ë‚´ëŠ” ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n",
    "ê·¸ë¦¬ê³  ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ ì§„í–‰í•´ì¤˜,  ë§Œì•½ ê·¸ë˜í”„ê°€ ì•„ë‹ˆë¼ë©´ 3ë²ˆê³¼4ë²ˆì„ ì¤‘ì ìœ¼ë¡œ ì§„í–‰í•´ì¤˜\n",
    "ëŒ€ë‹µì˜ ë§ˆì§€ë§‰ì— í•´ë‹¹ ```ì´ë¯¸ì§€ë¥¼ ëŒ€í‘œí•˜ëŠ” ì„¤ëª…```ì„ ì‘ì„±í•´ì¤˜\"\"\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image_url\", \n",
    "                     \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_image}\"},}\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "base_folder = \"./ë©í/\"\n",
    "# ëª¨ë“  í•˜ìœ„ í´ë”ê¹Œì§€ íƒìƒ‰í•˜ì—¬ ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    # *_imagesë¡œ ëë‚˜ëŠ” í´ë”ë§Œ ì„ íƒ\n",
    "    if not root.endswith(\"_images\"):\n",
    "        continue\n",
    "    folder_name = os.path.basename(root)  \n",
    "    parent_dir = os.path.dirname(root)  \n",
    "    company_name = os.path.splitext(os.path.basename(parent_dir))[0]\n",
    "    output_file = os.path.join(parent_dir, f\"{folder_name.split('_images')[0]}_img.xlsx\")\n",
    "    image_data_list = []\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith((\".jpg\",)):  # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì\n",
    "            image_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                summary = analyze_image(image_path, company_name)\n",
    "                print(f\"âœ… ë¶„ì„ ì™„ë£Œ: {image_path}\")\n",
    "                print(summary)\n",
    "                image_data_list.append([summary, summary])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {image_path} - {str(e)}\")\n",
    "    if not image_data_list:\n",
    "        continue\n",
    "    new_df = pd.DataFrame(image_data_list, columns=[\"embedding\", \"text\"])\n",
    "    new_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "    print(f\"âœ… '{output_file}' íŒŒì¼ì— ê²°ê³¼ ì €ì¥ ì™„ë£Œ.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬\n",
    "* Roughí•˜ê²Œ ì €ì¥ëœ txt íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ GPT 4o -minië¥¼ í™œìš©í•´ í…ìŠ¤íŠ¸ì™€ í‘œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ëŠ” ì‘ì—… ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "## ë°ì´í„° ì¶”ì¶œìš© í•¨ìˆ˜ ì„ ì–¸\n",
    "def extract_data(text_data_list):\n",
    "    # ë”•ì…”ë„ˆë¦¬ key \"embedding_data\"ì™€ \"table_title\"ì— ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥\n",
    "    table_embeddings = defaultdict(list)\n",
    "    table_title = \"\"\n",
    "    excel_data =[]\n",
    "    for i, text in enumerate(text_data_list):\n",
    "        text_data, table_data = text.split(\"í‘œ ë°ì´í„°\", 1)\n",
    "        for txt in text_data.split(\"\\n\\n\"):\n",
    "            if not txt.strip():\n",
    "                continue\n",
    "            excel_data.append([txt, txt])\n",
    "        for delimiter in [\"### ê·¸ë˜í”„(ë„í‘œ) ë°ì´í„°\", \"### ê·¸ë˜í”„ (ë„í‘œ) ë°ì´í„°\", \"ê·¸ë˜í”„(ë„í‘œ) ë°ì´í„°\"]:\n",
    "            table_data = table_data.split(delimiter, 1)[0]\n",
    "        \n",
    "        for table in table_data.split(\"\\n\\n\"):\n",
    "            if not table.strip():  # ë¹ˆ í…Œì´ë¸” ê±´ë„ˆë›°ê¸°\n",
    "                continue\n",
    "\n",
    "            col_keywords = \"\"  # ê° í…Œì´ë¸”ë§ˆë‹¤ ì´ˆê¸°í™”\n",
    "            row_keywords = \"\"\n",
    "            rows = [row.strip() for row in table.split(\"\\n\") if row.strip()]\n",
    "\n",
    "            # ìœ íš¨í•œ í…Œì´ë¸”ì¸ì§€ í™•ì¸\n",
    "            if len(rows) < 2:\n",
    "                if len(rows[0]) > 5:  # ì œëª©ìœ¼ë¡œ íŒë‹¨\n",
    "                    if table_title not in table_embeddings[\"table_title\"]:\n",
    "                        table_title = rows[0].strip()\n",
    "                    else:\n",
    "                        table_title = \"\"\n",
    "                continue\n",
    "\n",
    "            if '|' in table:  # Markdown í‘œ í˜•ì‹ ì²˜ë¦¬\n",
    "                if table_title == \"\":  # ì œëª©ì´ ì—†ëŠ” ê²½ìš°\n",
    "                    table_title = rows[0]\n",
    "                    row_keywords = rows[1].split('|')\n",
    "                else:\n",
    "                    row_keywords = rows[0].split('|')\n",
    "                \n",
    "                row_keywords = ', '.join(row_keywords).strip()\n",
    "                for row in rows[1:]:\n",
    "                    if '---' not in row:  # êµ¬ë¶„ì„  ì œì™¸\n",
    "                        cols = row.split('|')\n",
    "                        if len(cols) > 1 and cols[1].strip():\n",
    "                            col_keywords += ', ' + cols[1].strip()\n",
    "\n",
    "            # ìˆ«ì ë¹„ì¤‘ì´ ë‚®ì€ ìª½ì„ í‚¤ì›Œë“œë¡œ ì„ íƒ\n",
    "            row_num_count = len(re.findall(r'\\d', row_keywords))\n",
    "            col_num_count = len(re.findall(r'\\d', col_keywords))\n",
    "            row_temp = row_num_count / max(1, len(row_keywords))\n",
    "            col_temp = col_num_count / max(1, len(col_keywords)) \n",
    "            keywords = col_keywords if row_temp >= col_temp else row_keywords\n",
    "            \n",
    "            # í‘œ ì œëª© ì •ë¦¬: \"í‘œ X.\" ë˜ëŠ” \"### í‘œ X:\" ì œê±°\n",
    "            table_title = re.sub(r\"^(\\#*\\**\\s*í‘œ\\s*\\d+[\\.\\:\\-]?\\s*)\", \"\", table_title).strip()\n",
    "            # ì œëª©ê³¼ í‚¤ì›Œë“œ ì¡°í•©\n",
    "            if keywords:\n",
    "                table_embedding = f\"{table_title}{keywords.strip()}\"\n",
    "            else:\n",
    "                table_embedding = table_title\n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            table_embeddings[\"table_title\"].append(table_title)\n",
    "            excel_data.append([table_embedding, table])\n",
    "            table_title = \"\"\n",
    "            \n",
    "    true_data = []\n",
    "    for ebd, data in excel_data:\n",
    "        if len(data) > 40:\n",
    "            true_data.append([ebd,data])\n",
    "    return true_data\n",
    "\n",
    "# OpenAI API ì„¤ì • ë° í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì§„í–‰(í‘œì™€ í…ìŠ¤íŠ¸ ë°ì´í„°)\n",
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  \n",
    ")\n",
    "def read_text_files_by_pages(base_folder):\n",
    "    text_files = {}\n",
    "    # ëª¨ë“  í•˜ìœ„ í´ë” í¬í•¨í•˜ì—¬ .txt íŒŒì¼ ì°¾ê¸°\n",
    "    for root, _, files in os.walk(base_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as in_file:\n",
    "                    content = in_file.read().strip()\n",
    "                    pages = content.split(\"\\n--- Page \")  # í˜ì´ì§€ êµ¬ë¶„ì ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  ì§„í–‰\n",
    "                    text_files[file_path] = [p.strip() for p in pages if p.strip()]  # ë¹ˆ ê°’ ì œê±°\n",
    "    return text_files\n",
    "\n",
    "def classify_text_with_gpt(page_content):\n",
    "\n",
    "    response =  client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œì—ì„œ ì¼ë°˜ í…ìŠ¤íŠ¸ì™€ í‘œ ë°ì´í„°, ê·¸ë˜í”„(ë„í‘œ)ë°ì´í„°ë¥¼ êµ¬ë¶„í•´ ì£¼ì„¸ìš” ì—†ìœ¼ë©´ ì—†ìŒì´ë¼ê³  ì•Œë ¤ì£¼ì„¸ìš”. í‘œëŠ” í–‰ì„ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•˜ê³ , í‘œë¥¼ ì—¬ëŸ¬ê°œë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤ë©´ ê°ê° ë¶„ë¦¬í•´ì¤˜, í‘œì˜ ë¹ˆì¹¸ì€ N/Aë¡œ ì±„ìš°ê³  ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì˜ˆì˜ê²Œ ì •ë¦¬í•´. í…ìŠ¤íŠ¸ëŠ” í…ìŠ¤íŠ¸ ë‚´ìš© ì „ì²´ë¥¼ ì‘ì„±í•˜ê³  í‘œë„ ë°ì´í„°ë¥¼ ë†“ì¹˜ì§€ë§ê³ , í‘œì™€ í…ìŠ¤íŠ¸ì— ëŒ€í•œ êµ¬ë¶„ì€ í‘œ ë°ì´í„°, í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ êµ¬ë¶„í•´.\"},\n",
    "            {\"role\": \"user\", \"content\": f\" \\n\\n{page_content}\"}\n",
    "        ],\n",
    "        max_tokens=6000\n",
    "    )\n",
    "    return response.choices[0].message.content \n",
    "\n",
    "\n",
    "# ì‹¤í–‰ ì½”ë“œ\n",
    "folder_path = \"./ë©í/í•œí™”ì†”ë£¨ì…˜\"  # ì—¬ê¸°ì— í´ë” ê²½ë¡œ ì…ë ¥\n",
    "files_with_pages = read_text_files_by_pages(folder_path)\n",
    "\n",
    "for file_path, pages in files_with_pages.items():\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]  # ì›ë³¸ íŒŒì¼ ì´ë¦„ (í™•ì¥ì ì œê±°)\n",
    "    output_file_name = f\"{base_name}_gpt_text.xlsx\"\n",
    "    save_dir = os.path.dirname(file_path)\n",
    "    output_path = os.path.join(save_dir, output_file_name)\n",
    "    text_data_list = []\n",
    "    # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆë›°ê¸°\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"âš ï¸ '{output_path}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        continue\n",
    "    for page in pages:\n",
    "        data = classify_text_with_gpt(page)\n",
    "        text_data_list.append(data)\n",
    "    print(f\"âœ… '{file_path}' íŒŒì¼ ë¶„ì„ ì™„ë£Œ.\")\n",
    "    true_data = extract_data(text_data_list)\n",
    "    df = pd.DataFrame(true_data, columns=[\"embedding\", \"text\"])\n",
    "    df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "    print(f\"âœ… '{output_path}' íŒŒì¼ì— ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ 2\n",
    "* ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶ˆí•„ìš”í•œ ë°ì´í„° ì‚­ì œ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "file_path = \"./ë©í\"\n",
    "def english_ratio(text):\n",
    "        if not isinstance(text, str):\n",
    "            return 0\n",
    "        clean_text = re.sub(r'[^a-zA-Zê°€-í£0-9\\s]', '', text)\n",
    "        clean_text = clean_text.replace(' ', '')\n",
    "        total_chars = len(clean_text)\n",
    "        english_chars = len(re.findall(r'[a-zA-Z]', clean_text))\n",
    "        return (english_chars / total_chars) * 100 if total_chars > 0 else 0\n",
    "    \n",
    "    \n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "    text = text.strip()  # ì•ë’¤ ê³µë°± ì œê±°\n",
    "    text = re.sub(r\"[\\t]\", \" \", text)  # íƒ­ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ë³€ê²½\n",
    "    text = re.sub(r\"\\n{2,}\", \" \", text)  # ì—°ì†ëœ ê°œí–‰(ë‘ ê°œ ì´ìƒ) â†’ ê³µë°±ìœ¼ë¡œ ë³€ê²½\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)  # ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€ê²½\n",
    "    return text\n",
    "\n",
    "def is_meaningless_text(text: str) -> bool:\n",
    "    if re.search(r\"^\\s*\\|.*\\|\", text, re.MULTILINE):\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        max_columns = max(len(line.split(\"|\")) for line in lines)\n",
    "        columns = [col.strip() if col.strip() else f\"col_{i}\" for i, col in enumerate(lines[0].split(\"|\"))]\n",
    "        if len(columns) < max_columns:\n",
    "            columns += [f\"col_{i}\" for i in range(len(columns), max_columns)]\n",
    "        data = []\n",
    "        try:\n",
    "            for line in lines[2:]:\n",
    "                values = [val.strip() if val.strip() else \"N/A\" for val in line.split(\"|\")]\n",
    "                if len(values) < max_columns:\n",
    "                    values += [\"N/A\"] * (max_columns - len(values))\n",
    "                data.append(values)\n",
    "            df = pd.DataFrame(data, columns=columns[:max_columns])\n",
    "            if df.empty:\n",
    "                return False\n",
    "            total_cells = df.size\n",
    "            na_count = (df == \"N/A\").sum().sum()\n",
    "            if total_cells > 0:  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì˜¤ë¥˜ ë°©ì§€\n",
    "                na_ratio_exact = (na_count / total_cells) * 100\n",
    "                #print(f\"N/A ë¹„ìœ¨: {na_ratio_exact:.2f}%\")\n",
    "                if na_ratio_exact >= 61:\n",
    "                    #print(df)\n",
    "                    return True\n",
    "            else:\n",
    "                print(\"âš  Warning: No valid cells to calculate N/A ratio!\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "remove_keywords = [\n",
    "    \"ì´ ìë£Œì— ê²Œì¬ëœ ë‚´ìš©ë“¤ì€ ë³¸ì¸ì˜ ì˜ê²¬ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆìœ¼ë©°\",\n",
    "    \"ë³¸ ìë£Œì— ê¸°ì¬ëœ ë‚´ìš©ë“¤ì€ ì‘ì„±ì ë³¸ì¸ì˜ ì˜ê²¬ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆìœ¼ë©°\",\n",
    "    \"ë³¸ ì¡°ì‚¬ë¶„ì„ìë£Œì— ê²Œì¬ëœ ë‚´ìš©ë“¤ì´ ë³¸ì¸ì˜ ì˜ê²¬ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆìœ¼ë©°\",\n",
    "    \"ì´ ìë£ŒëŠ” ì¡°ì‚¬ë¶„ì„ ë‹´ë‹¹ìê°€ ê°ê´€ì  ì‚¬ì‹¤ì— ê·¼ê±°í•´ ì‘ì„±í•˜ì˜€ìœ¼ë©°\",\n",
    "    \"ë™ ìë£ŒëŠ” ê¸°ê´€íˆ¬ìê°€ ë˜ëŠ” ì œ3ìì—ê²Œ ì‚¬ì „ì œê³µí•œ ì‚¬ì‹¤ì´ ì—†ìŠµë‹ˆë‹¤\",\n",
    "    \"ë³¸ ìë£Œë¥¼ ì‘ì„±í•œ ì• ë„ë¦¬ìŠ¤íŠ¸ëŠ” ì™¸ë¶€ ì••ë ¥ì´ë‚˜ ê°„ì„­ì„ ë°›ì§€ ì•Šì•˜ìœ¼ë©°\",\n",
    "    \"ê³ ì§€ì‚¬í•­ ë³¸ ì¡°ì‚¬ë¶„ì„ìë£ŒëŠ” ë‹¹ì‚¬ì˜ ë¦¬ì„œì¹˜ì„¼í„°ê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ìë£Œ ë° ì •ë³´ë¡œë¶€í„° ì–»ì€ ê²ƒì´ë‚˜\",\n",
    "    \"ë³¸ ìë£ŒëŠ” íˆ¬ììì˜ íˆ¬ìë¥¼ ê¶Œìœ í•  ëª©ì ìœ¼ë¡œ ì‘ì„±ëœ ê²ƒì´ ì•„ë‹ˆë¼\",\n",
    "    \"ë³¸ ì¡°ì‚¬ë¶„ì„ìë£Œì—ëŠ” ì™¸ë¶€ì˜ ë¶€ë‹¹í•œ ì••ë ¥ì´ë‚˜ ê°„ì„­ ì—†ì´ ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ì˜ê²¬ì´ ì •í™•í•˜ê²Œ ë°˜ì˜ë˜ì—ˆìŒì„ í™•ì¸í•©ë‹ˆë‹¤\",\n",
    "    'ë‹¹ì‚¬ì˜ ê¸ˆìœµíˆ¬ìë¶„ì„ì‚¬ëŠ” ìë£Œì‘ì„±ì¼ í˜„ì¬ ë³¸ ìë£Œì— ê´€ë ¨í•˜ì—¬ ì¬ì‚°ì  ì´í•´ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤',\n",
    "    \"ì´ ìë£Œì— ê²Œì¬ëœ ë‚´ìš©ë“¤ì€ ì‘ì„±ìì˜ ì˜ê²¬ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆìœ¼ë©°\",\n",
    "    \"ì´ ë¬¸ì„œì˜ ë‚´ìš©ì´ ë³¸ì¸ì˜ ì˜ê²¬ì„ ë°˜ì˜í•˜ê³  ì™¸ë¶€ì˜ ì••ë ¥ ì—†ì´ ì‘ì„±ë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤\",\n",
    "    \"ë³¸ ì¡°ì‚¬ìë£ŒëŠ” ê³ ê°ì˜ íˆ¬ìì— ì •ë³´ë¥¼ ì œê³µí•  ëª©ì ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°\",\n",
    "    \"ë™ ìë£ŒëŠ” ì œê³µì‹œì  í˜„ì¬ ê¸°ê´€íˆ¬ìê°€ ë˜ëŠ” ì œ3ìì—ê²Œ ì‚¬ì „ ì œê³µí•œ ì‚¬ì‹¤ì´ ì—†ìŠµë‹ˆë‹¤\",\n",
    "    \"ì „ì¼ê¸°ì¤€ ë‹¹ì‚¬ì—ì„œ 1% ì´ìƒ ë³´ìœ í•˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤\",\n",
    "    \"ì¡°ì‚¬ë¶„ì„ìë£Œê°€ ê³ ê°ì˜ íˆ¬ì ì°¸ê³ ìš©ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŒì„ ëª…ì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë‹¹ì‚¬ëŠ” ìƒê¸° ëª…ì‹œí•œ ì‚¬í•­ ì™¸ ê³ ì§€í•´ì•¼ í•˜ëŠ” íŠ¹ë³„í•œ ì´í•´ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "    \"ì´ ì´ë¯¸ì§€ëŠ” í‘œ í˜•íƒœì˜ ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê·¸ë˜í”„ëŠ” í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\",\n",
    "    \"ìë£Œ: CJì œì¼ì œë‹¹, DSíˆ¬ìì¦ê¶Œ ë¦¬ì„œì¹˜ì„¼í„° ì¶”ì •\",\n",
    "    \"ë§¤ë„ -10% ì´í•˜ì˜ ì£¼ê°€í•˜ë½ì´ ì˜ˆìƒë˜ëŠ” ê²½ìš° ë¹„ì¤‘ì¶•ì†Œ\",\n",
    "    \"ë™ ìë£Œì— ê²Œì¬ëœ ë‚´ìš©ë“¤ì€ ì™¸ë¶€ì˜ ì••ë ¥ì´ë‚˜ ë¶€ë‹¹í•œ ê°„ì„­ì—†ì´ ë³¸ì¸ì˜ ì˜ê²¬ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ í™•ì¸í•©ë‹ˆë‹¤\",\n",
    "    \"ì¢…ëª©íˆ¬ìì˜ê²¬ì€ í–¥í›„ 12ê°œì›”ê°„ ì¶”ì²œì¼ ì¢…ê°€ëŒ€ë¹„ í•´ë‹¹ì¢…ëª©ì˜ ì˜ˆìƒ ëª©í‘œìˆ˜ìµë¥ ì„ ì˜ë¯¸í•¨.\",\n",
    "    \"Sell(ë§¤ë„): KOSPI ëŒ€ë¹„ ê¸°ëŒ€ìˆ˜ìµë¥  -10% ì´í•˜\",\n",
    "    \"ë³¸ ìë£Œë¥¼ ì‘ì„±í•œ ì• ë„ë¦¬ìŠ¤íŠ¸ëŠ” ìë£Œì‘ì„±ì¼ í˜„ì¬ ì¶”ì²œ ì¢…ëª©ê³¼ ì¬ì‚°ì  ì´í•´ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤\",\n",
    "    \"ì¤‘ë¦½ : ì—…ì¢…ë‚´ ì»¤ë²„ë¦¬ì§€ ì—…ì²´ë“¤ì˜ íˆ¬ìì˜ê²¬ì´ ì‹œê°€ì´ì•¡ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë¦½ì ì¼ ê²½ìš° ì¤‘ë¦½ : í–¥í›„ 6ê°œì›” ìˆ˜ìµë¥ ì´ -10% ~-20%\",\n",
    "    \"ë‹¹ì‚¬ëŠ” ë™ ìë£Œë¥¼ ê¸°ê´€íˆ¬ìì ë˜ëŠ” ì œ3ìì—ê²Œ ì‚¬ì „ ì œê³µí•œ ì‚¬ì‹¤ì´ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "    \"ë³¸ ì¡°ì‚¬ë¶„ì„ìë£ŒëŠ” ë‹¹ì‚¬ì˜ ë¦¬ì„œì¹˜ì„¼í„°ê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ìë£Œ ë° ì •ë³´ë¡œë¶€í„° ì–»ì€ ê²ƒì´ë‚˜, ë‹¹ì‚¬ê°€ ê·¸ ì •í™•ì„±ì´ë‚˜ ì™„ì „ì„±ì„ ë³´ì¥í•  ìˆ˜ ì—†ê³ , í†µì§€ ì—†ì´ ì˜ê²¬ì´ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì™¸ë¶€ì˜ ì••ë ¥ì´ë‚˜ ê°„ì„­ ì—†ì´ ì‹ ì˜ì„±ì‹¤í•˜ê²Œ ì‘ì„±ë˜ì—ˆìŒì„ ë°í™ë‹ˆë‹¤.\",\n",
    "    \"ë‹¹ ë³´ê³ ì„œ ê³µí‘œì¼ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ê¸°ì—…ê³¼ ê´€ë ¨í•˜ì—¬, íšŒì‚¬ëŠ” í•´ë‹¹ ì¢…ëª©ì„ 1%ì´ìƒ ë³´ìœ í•˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\",\n",
    "    \"ë³¸ ë¶„ì„ìë£ŒëŠ” íˆ¬ììì˜ ì¦ê¶Œíˆ¬ìë¥¼ ë•ê¸° ìœ„í•œ ì°¸ê³ ìë£Œì´ë©°\",\n",
    "    \"ë‹¹ì‚¬ëŠ” ê°œë³„ ì¢…ëª©ì— ëŒ€í•´ í–¥í›„ 1 ë…„ê°„ +15% ì´ìƒì˜ ì ˆëŒ€ìˆ˜ìµë¥ ì´ ê¸°ëŒ€ë˜ëŠ” ì¢…ëª©ì— ëŒ€í•´ Buy(ë§¤ìˆ˜) ì˜ê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤.\",\n",
    "    \"ë‹¹ì‚¬ëŠ” ì‚°ì—…ì— ëŒ€í•´ í–¥í›„ 1 ë…„ê°„ í•´ë‹¹ ì—…ì¢…ì˜ ìˆ˜ìµë¥ ì´ ê³¼ê±° ìˆ˜ìµë¥ ì— ë¹„í•´ ì–‘í˜¸í•œ íë¦„ì„ ë³´ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ê²½ìš°ì— Positive(ê¸ì •ì ) ì˜ê²¬ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
    "    \"ì´ ì¡°ì‚¬ìë£ŒëŠ” ë‹¹ì‚¬ ë¦¬ì„œì¹˜ì„¼í„°ê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ìë£Œ ë° ì •ë³´ë¡œë¶€í„° ì–»ì–´ì§„ ê²ƒì´ë‚˜,\",\n",
    "    \"Hold(ë³´ìœ ): KOSPI ëŒ€ë¹„ ê¸°ëŒ€ìˆ˜ìµë¥  -10~10%\",\n",
    "    \"ìë£Œì˜ ì‘ì„±ê³¼ ê´€ë ¨í•˜ì—¬ ì™¸ë¶€ì˜ ì••ë ¥ì´ë‚˜ ë¶€ë‹¹í•œ ê°„ì„­ì„ ë°›ì§€ ì•Šì•˜ìœ¼ë©°\",\n",
    "]\n",
    "\n",
    "for root, _, files in os.walk(file_path):   \n",
    "    for file in files:\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            file_full_path = os.path.join(root, file)\n",
    "            df = pd.read_excel(file_full_path, engine=\"openpyxl\")\n",
    "            # í…ìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜\n",
    "            df['embedding'] = df['embedding'].apply(clean_text)\n",
    "            df['text'] = df['text'].apply(clean_text)\n",
    "            df['embedding'] = df['embedding'].astype(str).str.replace('í…ìŠ¤íŠ¸ ë°ì´í„°', '', regex=False)\n",
    "            df['embedding'] = df['embedding'].str.replace(r'#+', '#', regex=True)\n",
    "            df['text'] = df['text'].astype(str).str.replace('í…ìŠ¤íŠ¸ ë°ì´í„°', '', regex=False)\n",
    "            df['text'] = df['text'].str.replace(r'#+', '#', regex=True) \n",
    "            df['embedding'] = df['embedding'].astype(str).str.replace('COMPANY REPORT', '', regex=False)\n",
    "            df['text'] = df['text'].astype(str).str.replace('COMPANY REPORT', '', regex=False)\n",
    "            for remove_keyword in remove_keywords:\n",
    "                df = df[~df[\"embedding\"].str.contains(remove_keyword, na=False, regex=False)]\n",
    "                df = df[~df[\"text\"].str.contains(remove_keyword, na=False, regex=False)] \n",
    "            df['english_ratio'] = df['text'].apply(english_ratio)\n",
    "            df = df[(df['english_ratio'] < 98) & (df['text'].astype(str).str.len() > 40)]\n",
    "            df = df[~df[\"text\"].apply(is_meaningless_text)]\n",
    "            df = df.drop(columns=['english_ratio'])\n",
    "            df.to_excel(file_full_path, index=False, engine='openpyxl')\n",
    "            print(f\"Checked file: {file_full_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
